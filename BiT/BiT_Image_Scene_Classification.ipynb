{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BiT Image Scene Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPXsSkM-ZwoE"
      },
      "source": [
        "## Initial Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nymcf4_NZoN9",
        "outputId": "aeabc108-a345-4964-8f73-d12f62fd4297"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Mar  7 13:54:52 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.39       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QihESoGZ7Sf"
      },
      "source": [
        "## Data Gathering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHW45NA5ZzdI"
      },
      "source": [
        "!wget -q http://data.vision.ee.ethz.ch/ihnatova/camera_scene_detection_train.zip\n",
        "!unzip -qq camera_scene_detection_train.zip"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0hdgvokaCAD"
      },
      "source": [
        "## Importing Tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLmj1NRCZ_HN",
        "outputId": "d0d4cc24-a92a-4d2d-8fc8-264fbc74afe7"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qex9JCFFaJLC"
      },
      "source": [
        "## Importing necessary modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rqesDynaFv_"
      },
      "source": [
        "from imutils import paths\n",
        "from pprint import pprint\n",
        "from collections import Counter\n",
        "import tensorflow_hub as hub\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import load_model, Sequential\n",
        "from tensorflow.keras.layers import *\n",
        "import matplotlib.pyplot as plt\n",
        "import re \n",
        "\n",
        "import numpy as np\n",
        "np.random.seed(42)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiowNJkmaNzP"
      },
      "source": [
        "## Data Parsing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ti-JHxMzaLiZ",
        "outputId": "34f47d72-39bb-432e-b929-6fa5a43c7855"
      },
      "source": [
        "image_paths = list(paths.list_images(\"training\"))\n",
        "np.random.shuffle(image_paths)\n",
        "image_paths[:5]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['training/27_Backlight/71.jpg',\n",
              " 'training/22_Stage_concert/296.jpg',\n",
              " 'training/11_Snow/336.jpg',\n",
              " 'training/15_Sunset_Sunrise/96.jpg',\n",
              " 'training/24_Candle_light/101.jpg']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnDfKZsKbQZz"
      },
      "source": [
        "## Counting number of images for each classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACH7l2U6aSKf",
        "outputId": "928a621d-a965-46a5-f0e2-4922f26b5320"
      },
      "source": [
        "labels = []\n",
        "for image_path in image_paths:\n",
        "    label = image_path.split(\"/\")[1]\n",
        "    labels.append(label)\n",
        "class_count = Counter(labels) \n",
        "pprint(class_count)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counter({'4_Dog': 502,\n",
            "         '14_Architecture': 463,\n",
            "         '15_Sunset_Sunrise': 420,\n",
            "         '7_Food': 400,\n",
            "         '5_Cat': 373,\n",
            "         '6_Macro': 357,\n",
            "         '9_Mountain': 352,\n",
            "         '11_Snow': 344,\n",
            "         '12_Landscape': 335,\n",
            "         '8_Beach': 334,\n",
            "         '10_Waterfall': 326,\n",
            "         '2_Group_portrait': 324,\n",
            "         '22_Stage_concert': 322,\n",
            "         '16_Blue_Sky': 322,\n",
            "         '1_Portrait': 319,\n",
            "         '23_Fireworks': 317,\n",
            "         '21_Night_shot': 316,\n",
            "         '24_Candle_light': 313,\n",
            "         '30_Computer_Screens': 308,\n",
            "         '17_Cloudy_Sky': 307,\n",
            "         '28_Text_Documents': 306,\n",
            "         '26_Indoor': 298,\n",
            "         '13_Underwater': 296,\n",
            "         '19_Autumn_leaves': 295,\n",
            "         '18_Greenery': 294,\n",
            "         '3_Kids': 281,\n",
            "         '27_Backlight': 280,\n",
            "         '20_Flower': 279,\n",
            "         '25_Neon_lights': 269,\n",
            "         '29_QR_images': 245})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wmBH9iUbdxw"
      },
      "source": [
        "TRAIN_SPLIT = 0.9\n",
        "BATCH_SIZE = 128\n",
        "AUTO = tf.data.AUTOTUNE\n",
        "EPOCHS = 100"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FrAeGCObjOp",
        "outputId": "208b92c7-67b5-4394-fbb7-a81452a0826f"
      },
      "source": [
        "i = int(len(image_paths) * TRAIN_SPLIT)\n",
        "\n",
        "train_paths = image_paths[:i]\n",
        "train_labels = labels[:i]\n",
        "validation_paths = image_paths[i:]\n",
        "validation_labels = labels[i:]\n",
        "\n",
        "print(len(train_paths), len(validation_paths))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8907 990\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4cYliLjdKuQ",
        "outputId": "6c57c8a2-99d4-4707-9ba0-fc3b82b5eb0f"
      },
      "source": [
        "le = LabelEncoder()\n",
        "train_labels_le = le.fit_transform(train_labels)\n",
        "validation_labels_le = le.transform(validation_labels)\n",
        "print(train_labels_le[:5])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[18 13  1  5 15]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVOJVxmldPrj"
      },
      "source": [
        "## DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwFtWRDdemiP"
      },
      "source": [
        "train_ds = tf.data.Dataset.from_tensor_slices((train_paths, train_labels_le))\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((validation_paths, validation_labels_le))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6-ny24Fe8XD"
      },
      "source": [
        "Since the dataset has class imbalance issue, it's good to supply class weights while training the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OsfW8Iee4Z7"
      },
      "source": [
        "# Reference: https://www.pyimagesearch.com/2019/02/18/breast-cancer-classification-with-keras-and-deep-learning/\n",
        "\n",
        "trainLabels = to_categorical(train_labels_le)\n",
        "classTotals = trainLabels.sum(axis=0)\n",
        "classWeight = dict()\n",
        "# loop over all classes and calculate the class weight\n",
        "for i in range(0, len(classTotals)):\n",
        "\tclassWeight[i] = classTotals.max() / classTotals[i]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_0rDJUffQJ0"
      },
      "source": [
        "## Set dataset-dependent hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irdo0Ovre-6k"
      },
      "source": [
        "IMAGE_SIZE = \"> 96 x 96 px\" #@param [\"=<96x96 px\",\"> 96 x 96 px\"]\n",
        "DATASET_SIZE = \"\\u003C20k examples\" #@param [\"<20k examples\", \"20k-500k examples\", \">500k examples\"]\n",
        "\n",
        "if IMAGE_SIZE == \"=<96x96 px\":\n",
        "  RESIZE_TO = 260\n",
        "  CROP_TO = 224\n",
        "else:\n",
        "  RESIZE_TO = 260\n",
        "  CROP_TO = 224\n",
        "\n",
        "if DATASET_SIZE == \"<20k examples\":\n",
        "  SCHEDULE_LENGTH = 500\n",
        "  SCHEDULE_BOUNDARIES = [200, 300, 400]\n",
        "elif DATASET_SIZE == \"20k-500k examples\":\n",
        "  SCHEDULE_LENGTH = 10000\n",
        "  SCHEDULE_BOUNDARIES = [3000, 6000, 9000]\n",
        "else:\n",
        "  SCHEDULE_LENGTH = 20000\n",
        "  SCHEDULE_BOUNDARIES = [6000, 12000, 18000]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UZAwuIzfaxB"
      },
      "source": [
        "## Preprocessing helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyCnWTtLfSkc"
      },
      "source": [
        "SCHEDULE_LENGTH = SCHEDULE_LENGTH * 512 / BATCH_SIZE\n",
        "\n",
        "\n",
        "@tf.function  \n",
        "def preprocess_train(image_path, label):\n",
        "    image = tf.io.read_file(image_path)\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    image = tf.image.resize(image, (RESIZE_TO, RESIZE_TO))\n",
        "    image = tf.image.random_crop(image, [CROP_TO, CROP_TO, 3])\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "    return (image, label)\n",
        "\n",
        "@tf.function\n",
        "def preprocess_test(image_path, label):\n",
        "    image = tf.io.read_file(image_path)\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    image = tf.image.resize(image, (RESIZE_TO, RESIZE_TO))\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "    return (image, label)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBvXyPvygBRg"
      },
      "source": [
        "DATASET_NUM_TRAIN_EXAMPLES=len(train_paths)\n",
        "NUM_CLASSES=30\n",
        "STEPS_PER_EPOCH = 140"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cE8mv5MC8M6m"
      },
      "source": [
        "## Create data pipelines for training and testing:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLvNGJRffkMp"
      },
      "source": [
        "pipeline_train = (train_ds\n",
        "                  .shuffle(10000)\n",
        "                  .repeat(int(SCHEDULE_LENGTH * BATCH_SIZE / DATASET_NUM_TRAIN_EXAMPLES * STEPS_PER_EPOCH) + 1 + 50)  # repeat dataset_size / num_steps\n",
        "                  .map(preprocess_train, num_parallel_calls=8)\n",
        "                  .batch(BATCH_SIZE)\n",
        "                  .prefetch(2))\n",
        "\n",
        "pipeline_test = (val_ds.map(preprocess_test, num_parallel_calls=1)\n",
        "                  .batch(BATCH_SIZE)\n",
        "                  .prefetch(2))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLOH_vTa7DwS"
      },
      "source": [
        "## # Load model into KerasLayer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJAYmmPk6-ch"
      },
      "source": [
        "model_url = \"https://tfhub.dev/google/bit/m-r50x1/1\"\n",
        "module = hub.KerasLayer(model_url, trainable=True)\n",
        "\n",
        "# module = Sequential([\n",
        "#     hub.KerasLayer(model_url, trainable=True), \n",
        "#     Dense(NUM_CLASSES, kernel_initializer='zeros')\n",
        "#     ])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WavQhAwH63sj"
      },
      "source": [
        "## BiT Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xk4Wb-Oe6mNg"
      },
      "source": [
        "class MyBiTModel(tf.keras.Model):\n",
        "\n",
        "  def __init__(self, num_classes, module):\n",
        "    super().__init__()\n",
        "\n",
        "    self.num_classes = num_classes\n",
        "    self.head = tf.keras.layers.Dense(num_classes, kernel_initializer='zeros')\n",
        "    self.bit_model = module\n",
        "  \n",
        "  def call(self, images):\n",
        "    # No need to cut head off since we are using feature extractor model\n",
        "    bit_embedding = self.bit_model(images)\n",
        "    return self.head(bit_embedding)\n",
        "\n",
        "model = MyBiTModel(num_classes=NUM_CLASSES, module=module)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nEdPuY18AaH"
      },
      "source": [
        "## Defining optimiser and loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpLbvjd7fuSn"
      },
      "source": [
        "lr = 0.003 * BATCH_SIZE / 512 \n",
        "\n",
        "# Decay learning rate by a factor of 10 at SCHEDULE_BOUNDARIES.\n",
        "lr_schedule = tf.keras.optimizers.schedules.PiecewiseConstantDecay(boundaries=SCHEDULE_BOUNDARIES, \n",
        "                                                                   values=[lr, lr*0.1, lr*0.01, lr*0.001])\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule, momentum=0.9)\n",
        "\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMFbZfZf6E0C"
      },
      "source": [
        "model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiiwpAAm78ph"
      },
      "source": [
        "## Setting the callback"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brUqtmVS6JBQ"
      },
      "source": [
        "train_callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\n",
        "]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsVke5kj76PK"
      },
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zq0PQL5N6Qdx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b657010-8f3f-4b9b-e0ab-b06d38505353"
      },
      "source": [
        "history = model.fit(\n",
        "    pipeline_train,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    steps_per_epoch=STEPS_PER_EPOCH,\n",
        "    epochs= int(SCHEDULE_LENGTH / STEPS_PER_EPOCH),  \n",
        "    class_weight=classWeight,\n",
        "    validation_data=pipeline_test,\n",
        "    callbacks=train_callbacks)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/14\n",
            "140/140 [==============================] - 360s 2s/step - loss: 1.1844 - accuracy: 0.8109 - val_loss: 0.1188 - val_accuracy: 0.9596\n",
            "Epoch 2/14\n",
            "140/140 [==============================] - 312s 2s/step - loss: 0.0672 - accuracy: 0.9871 - val_loss: 0.0973 - val_accuracy: 0.9717\n",
            "Epoch 3/14\n",
            "140/140 [==============================] - 312s 2s/step - loss: 0.0350 - accuracy: 0.9952 - val_loss: 0.0951 - val_accuracy: 0.9697\n",
            "Epoch 4/14\n",
            "140/140 [==============================] - 312s 2s/step - loss: 0.0334 - accuracy: 0.9960 - val_loss: 0.0951 - val_accuracy: 0.9697\n",
            "Epoch 5/14\n",
            "140/140 [==============================] - 313s 2s/step - loss: 0.0322 - accuracy: 0.9966 - val_loss: 0.0950 - val_accuracy: 0.9697\n",
            "Epoch 6/14\n",
            "140/140 [==============================] - 312s 2s/step - loss: 0.0326 - accuracy: 0.9961 - val_loss: 0.0949 - val_accuracy: 0.9697\n",
            "Epoch 7/14\n",
            "140/140 [==============================] - 312s 2s/step - loss: 0.0341 - accuracy: 0.9953 - val_loss: 0.0949 - val_accuracy: 0.9697\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5iZMoNm71ua"
      },
      "source": [
        "## Plotting the Graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4yCZrew6pvN"
      },
      "source": [
        "def plot_hist(hist):\n",
        "    plt.plot(hist.history[\"accuracy\"])\n",
        "    plt.plot(hist.history[\"val_accuracy\"])\n",
        "    plt.plot(hist.history[\"loss\"])\n",
        "    plt.plot(hist.history[\"val_loss\"])\n",
        "    plt.title(\"Training Progress\")\n",
        "    plt.ylabel(\"Accuracy/Loss\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.legend([\"train_acc\", \"val_acc\", \"train_loss\", \"val_loss\"], loc=\"upper left\")\n",
        "    plt.show()"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "954TitfW7yUT"
      },
      "source": [
        "## Evaluate our Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "jHtma-aq6t0s",
        "outputId": "ed9de04b-5703-49a1-b783-1e0f5649c6ba"
      },
      "source": [
        "accuracy = model.evaluate(pipeline_test)[1] * 100\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy))\n",
        "plot_hist(history)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8/8 [==============================] - 8s 996ms/step - loss: 0.0973 - accuracy: 0.9717\n",
            "Accuracy: 97.17%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FSgc3VqLInq"
      },
      "source": [
        "## Saving the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e14fCkXvDb9l",
        "outputId": "85bd048b-6090-4467-8b14-dc49703d9db7"
      },
      "source": [
        "model.save('bit_image_scene')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: bit_image_scene/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: bit_image_scene/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTaIkQ7WD8qE",
        "outputId": "341c0ba0-4de2-4958-dbc5-e5195e597350"
      },
      "source": [
        "!du -lh bit_image_scene"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4.0K\tbit_image_scene/assets\n",
            "180M\tbit_image_scene/variables\n",
            "191M\tbit_image_scene\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoC-ag1cEMyY"
      },
      "source": [
        "## Inference Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lf64IwFEBoR"
      },
      "source": [
        "!wget -q https://data.vision.ee.ethz.ch/ihnatova/camera_scene_detection_validation.zip\n",
        "!unzip -qq camera_scene_detection_validation.zip"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foHQO-qhEP8X"
      },
      "source": [
        "# https://stackoverflow.com/a/2669120/7636462\n",
        "def sorted_nicely(l): \n",
        "    \"\"\" Sort the given iterable in the way that humans expect.\"\"\" \n",
        "    convert = lambda text: int(text) if text.isdigit() else text \n",
        "    alphanum_key = lambda key: [ convert(c) for c in re.split('([0-9]+)', key) ] \n",
        "    return sorted(l, key = alphanum_key)\n",
        "\n",
        "def preprocess_image(image_path):\n",
        "    image = tf.io.read_file(image_path)\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    image = tf.image.resize(image, (224, 224))\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "    return image"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCi7PLDJErW4",
        "outputId": "bec1b871-1d61-4c6c-e9fa-73136db9371f"
      },
      "source": [
        "test_image_paths = sorted_nicely(list(paths.list_images(\"images\")))\n",
        "print(f\"Total test images: {len(test_image_paths)}\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total test images: 600\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qAN5nLtEtWl",
        "outputId": "1c352a27-0763-498c-ae16-7dafabe30266"
      },
      "source": [
        "test_image_paths[:5]"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['images/0.jpg',\n",
              " 'images/1.jpg',\n",
              " 'images/2.jpg',\n",
              " 'images/3.jpg',\n",
              " 'images/4.jpg']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DdGUINFrEu4W",
        "outputId": "7eb28e8f-1e45-4f68-8c0f-026fd770b715"
      },
      "source": [
        "classifier_model = load_model(\"bit_image_scene\")\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices(test_image_paths)\n",
        "test_ds = (\n",
        "    test_ds\n",
        "    .map(preprocess_image)\n",
        "    .batch(BATCH_SIZE)\n",
        ")\n",
        "\n",
        "test_predictions = np.argmax(classifier_model.predict(test_ds), 1)\n",
        "test_predictions = le.inverse_transform(test_predictions)\n",
        "print(test_predictions.shape)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(600,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KV7eXPz3EzEa",
        "outputId": "7a6b4291-90f5-4a1e-f645-c7048394a778"
      },
      "source": [
        "test_predictions_num = list(map(lambda x: x.split(\"_\")[0], test_predictions.tolist()))\n",
        "print(test_predictions_num[:5])\n",
        "with open('results.txt', \"w\") as myfile:\n",
        "    myfile.write('\\n'.join(list(test_predictions_num)))\n",
        "!head -5 results.txt"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['9', '21', '27', '4', '28']\n",
            "9\n",
            "21\n",
            "27\n",
            "4\n",
            "28\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fBdaCm3E1Xd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd9c6717-53a6-4c1f-a508-47f2f4863d1c"
      },
      "source": [
        "converter = tf.lite.TFLiteConverter.from_saved_model(\"bit_image_scene\")\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.target_spec.supported_ops = [\n",
        "    tf.lite.OpsSet.TFLITE_BUILTINS,\n",
        "    tf.lite.OpsSet.SELECT_TF_OPS \n",
        "]\n",
        "tflite_model = converter.convert()\n",
        "open(\"bit_image_scene.tflite\", 'wb').write(tflite_model)\n",
        "print('Model size is %f MBs.' % (len(tflite_model) / 1024 / 1024.0))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model size is 23.269440 MBs.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYshtd-sRnxc",
        "outputId": "79656438-e99f-44e7-facd-9c0bb7f16b6c"
      },
      "source": [
        "interpreter = tf.lite.Interpreter(model_path = 'bit_image_scene.tflite')\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "print(\"Input Shape:\", input_details[0]['shape'])\n",
        "print(\"Input Type:\", input_details[0]['dtype'])\n",
        "print(\"Output Shape:\", output_details[0]['shape'])\n",
        "print(\"Output Type:\", output_details[0]['dtype'])"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Shape: [  1 224 224   3]\n",
            "Input Type: <class 'numpy.float32'>\n",
            "Output Shape: [ 1 30]\n",
            "Output Type: <class 'numpy.float32'>\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}